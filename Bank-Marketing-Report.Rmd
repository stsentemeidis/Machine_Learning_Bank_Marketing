---
title: "Predicting Term Deposits"
author: "MBD | ADVANCED R | June 2019"
always_allow_html: yes
output:
  html_document:
    theme: yeti
    code_folding: 'none' 
  github_document: default
params: 
  shiny: TRUE
runtime: shiny
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r Load Libraries, echo = FALSE, include = FALSE}
source('scripts/install_packages.R')

if (is.null(webshot:::find_phantom())) {
  webshot::install_phantomjs()
}
```

``` {r Load Prepared Data, echo = FALSE}
load('data_output/RMarkdown_Objects.RData')
```

``` {r Palettes, echo = FALSE}
BarFillColor <- "#330066"
HBarFillColor <- "#000099"
BarLineColor <- "#FFFAFA"
MissingColor <- "#FF6666"

color1 = 'white'
color2 = 'black'
color3 = 'black'
color4 = 'darkorchid3'
font1 = 'Impact'
font2 = 'Helvetica'
```

</br>

***

## Bank Marketing Dataset  

The *Bank Marketing* dataset contains the **direct marketing campaigns of a Portuguese banking institution**. The original dataset can be found on [Kaggle](https://www.kaggle.com/henriqueyamahata/bank-marketing).  

All the files of this project are saved in a [GitHub repository](https://github.com/ashomah/Bank-Marketing).  

The dataset consists in:  

* **Train Set** with `r format(nrow(raw_train), big.mark=',')` observations with `r length(raw_train)-1` features and the target `y`.  
* **Test Set** with `r format(nrow(raw_test), big.mark=',')` observations with `r length(raw_test)` features. The `y` column will be added to the Test Set, with NAs, to ease the pre-processing stage.  

This project aims to predict if a customer will subscribe to a bank term deposit, based on its features and call history of previous marketing campaigns.  

</br>

***

## Packages  

<span style="color:red">This analysis requires these R packages:</span>  

* Data Manipulation: `data.table`, `dplyr`, `tibble`, `tidyr`  
    
* Plotting: `corrplot`, `GGally`, `ggmap`, `ggplot2`, `grid`, `gridExtra`  
    
* Machine Learning: `caret`, `dbscan`, `glmnet`, `leaderCluster`, `MLmetrics`, `ranger`, `xgboost`  

* Multithreading: `doMC`, `doParallel`, `factoextra`, `foreach`, `parallel`  

* Reporting: `kableExtra`, `knitr`, `RColorBrewer`, `shiny`, and... `beepr`.  

These packages are installed and loaded if necessary by the main script.

</br>

***
 
## Data Loading  

The data seems to be pretty clean, the variables being a combination of integers and factors with no null values.  

``` {r NAs, echo = FALSE, collapse=TRUE}
# Check if contains NAs ----
na_count <-
  sapply(raw_train, function(y)
    sum(length(which(is.na(
      y
    )))))
na_count <- data.frame(na_count)
na_count$perc <- round(na_count$na_count / nrow(raw_train) * 100, 2)
print(paste0(nrow(na_count[na_count$na_count != 0,]), ' columns of the Train Set have NAs.'))

na_count <-
  sapply(raw_test, function(y)
    sum(length(which(is.na(
      y
    )))))
na_count <- data.frame(na_count)
na_count$perc <- round(na_count$na_count / nrow(raw_test) * 100, 2)
print(paste0(nrow(na_count[na_count$na_count != 0,]), ' columns of the Test Set have NAs.'))
```

</br>

As this analysis is a classification, the target `y` has to be set as factor. The structures of the datasets after initial preparation are:  

``` {r Structure Train, echo = FALSE, collapse = TRUE}
str_train <- capture.output(str(bank_train))
cat('Structure of the Train Set:')
cat(str_train, sep='\n')
```

``` {r Structure Test, echo = FALSE, collapse = TRUE}
str_test <- capture.output(str(bank_test))
cat('Structure of the Test Set:')
cat(str_test, sep='\n')
```

</br>

***

## Exploratory Data Analysis  

The target of this analysis is the variable `y`. This boolean indicates whether the customer has acquiered a bank term deposit account. With `r format(length(bank_train[bank_train$y == 'No', 'y'])/nrow(bank_train)*100, nsmall = 1, digits=2)`% of the customers having subscribed to this product, we can say that our Train set is slightly unbalanced. We might want to try rebalancing our dataset later in this analysis, to ensure our model is performing properly for unknown data.  

The features of the dataset provide different type of information about the customers.  

</br>

* Variables giving **personal information** of the customers:  

  * **`age`** of the customer  
  Customers are between `r min(bank_train$age)` and `r max(bank_train$age)` years old, with a mean of `r mean(bank_train$age)` and a median of `r median(bank_train$age)`. The inter-quartile range is between `r quantile(bank_train$age, 0.25)` and `r quantile(bank_train$age, 0.75)`. We can also notice the presence of some outliers.  

  * **`job`** category of the customer  
  There are `r length(unique(bank_train$job))` categories of jobs with more than half belonging to `blue-collar`, `management` and `technicians`, followed by admin and services. Retired candidates form `r format(length(bank_train[bank_train$job == 'retired', 'job'])/nrow(bank_train)*100, digits=0, nsmall=0)`% of the dataset, self-emplyed and entrepreneur around 6% and unemployed, housemaid and students each around 2%. The candidate with unknow jobs form less than 1%.

  * **`marital`** status of the customer  
 `r format(length(bank_train[bank_train$marital == 'married', 'marital'])/nrow(bank_train)*100, digits=0, nsmall=0)`% are married, `r format(length(bank_train[bank_train$marital == 'single', 'marital'])/nrow(bank_train)*100, digits=0, nsmall=0)`% are single, the others are divorced.  

  * **`education`** level of the customer  
  `r format(length(bank_train[bank_train$education == 'secondary', 'education'])/nrow(bank_train)*100, digits=0, nsmall=0)`% of the customers went to secondary school, `r format(length(bank_train[bank_train$education == 'tertiary', 'marital'])/nrow(bank_train)*100, digits=0, nsmall=0)`% to tertiary school, `r format(length(bank_train[bank_train$education == 'primary', 'marital'])/nrow(bank_train)*100, digits=0, nsmall=0)`% to primary school. The education of the other customers remains unknown.  

</br>

* Variables related to **financial status** of the customers:  

  * **`default`** history  
  This boolean indicates if the customer has already defaulted. Only `r format(length(bank_train[bank_train$default == 1, 'default'])/nrow(bank_train)*100, digits=1, nsmall=1)`% of the customers have defaulted.  

  * **`balance`** of customer's account  
  The average yearly balance of the customer in euros. The variable is ranged from `r format(min(bank_train$balance), digits=0, nsmall=0, big.mark=',')` to `r format(max(bank_train$balance), digits=0, nsmall=0, big.mark=',')` with a mean of `r format(mean(bank_train$balance), digits=0, nsmall=0, big.mark=',')` and a median of `r format(median(bank_train$balance), digits=0, nsmall=0, big.mark=',')`. The data is highly right-skewed.  

  * **`housing` ** loan  
  This boolean indicates if the customer has a house loan. `r format(length(bank_train[bank_train$housing == 1, 'housing'])/nrow(bank_train)*100, digits=0, nsmall=0)`% of the customers have one.  

  * **`loan`**  
  This boolean indicates if the customer has a personal loan. `r format(length(bank_train[bank_train$loan == 1, 'loan'])/nrow(bank_train)*100, digits=0, nsmall=0)`% of the customers have one.  

</br>

* Variables related to **campaign interactions** with the customer:  

  * **`contact`** mode  
  How the customer was contacted, with `r format(length(bank_train[bank_train$contact == 'cellular', 'contact'])/nrow(bank_train)*100, digits=0, nsmall=0)`% on their mobile phone, and `r format(length(bank_train[bank_train$contact == 'telephone', 'contact'])/nrow(bank_train)*100, digits=0, nsmall=0)`% ona landline.  

  * **`day`**  
  This indicates on which day of the month the customer was contacted.  

  * **`month`**  
  This indicates on which month the customer was contacted. May seems to be the peak month with `r format(length(bank_train[bank_train$month == 'may', 'month'])/nrow(bank_train)*100, digits=0, nsmall=0)`% of the calls followed by June, July, and August.  

  * **`duration`** of the call  
  Last phone call duration in seconds. The average call lasts around `r format(mean(bank_train$duration)/60, digits=0, nsmall=0)` minutes. However, the longest call lasts `r format(max(bank_train$duration)/60/60, digits=1, nsmall=1)` hours.

  * **`campaign`**  
  Number of times the customer was contacted *during* this campaign. Customers can have been contacted up to `r max(bank_train$campaign)` times. Around `r format(length(bank_train[bank_train$campaign <= 2, 'campaign'])/nrow(bank_train)*100, digits=0, nsmall=0)`% were contacted twice or less.  

  * **`pdays`**  
  Number of days that passed after the customer was last contacted from a previous campaign. `-1` means that the client was not previously contacted and this is his first campaign. Around `r format(length(bank_train[bank_train$pdays == -1, 'pdays'])/nrow(bank_train)*100, digits=0, nsmall=0)`% of the candidates are newly campaign clients. The average time elapsed is `r format(mean(bank_train$pdays), digits=0, nsmall=0)` days.  

  * **`previous`** contacts  
  Number of contacts performed *before* this campaign. Majority of the customers were never contacted. Other customers have been contacted `r format(mean(bank_train[bank_train$previous != -1, 'previous']), digits=0, nsmall=0)` times on average, with a maximum of `r format(max(bank_train[bank_train$previous != -1, 'previous']), digits=0, nsmall=0)` times.  

  * **`poutcome`**
  This categorical variable indicates the outcome from a previous campaign, whether it was a success or a failure. About `r format(length(bank_train[bank_train$poutcome == 'success', 'poutcome'])/nrow(bank_train)*100, digits=0, nsmall=0)`% of the customers answered positively to previous campaigns.  

</br>

<iframe src ="http://ashomah.shinyapps.io/plot_eda" height=800px width=100% position="center" frameborder="0" />

**Test set**     
Looking at the `raw test` set which we will have to consider as our hold out dataset, we can see that the variables follow almost *similar summary results*.


</br>

***

## Data Preparation  


</br>

***

## Cross-Validation Strategy  

To validate the stability of our models, we will apply a 10-fold cross-validation, repeated 3 times.   
(*Note: for the stacking that is explained in the following part of the report we use another, more extensive cross validation approach*)

</br>

***

## Baseline  

``` {r Baseline Results, echo=FALSE, message = FALSE, fig.align = 'center'}
all_real_results[1:3,] %>%
  rownames_to_column('Model') %>%
  mutate(
    Sensitivity = cell_spec(
      format(Sensitivity, nsmall = 7),
      color = ifelse(order(Sensitivity) <= 1, 'white', 'ghostwhite'),
      font_size = ifelse(order(Sensitivity) <= 1, 12, 11),
      background = spec_color(
        order(Sensitivity),
        option = 'D',
        begin = 0.50,
        end = 0.98,
        direction = 1
      ),
      bold = ifelse(order(Sensitivity) <= 1, TRUE, FALSE)
    )
  ) %>%
  kable(escape = FALSE, align = 'r') %>% kable_styling(bootstrap_options = c('hover', 'condensed'))
```

</br>

***

## Feature Engineering

### A. Clusters

The first step of the Feature Engineering process is to create a new feature based on **clustering method**. First of all, we select the variable describing the clients: `age`, `job`, `mar`, `edu`, `def`, `bal`, `hou`, `loa`. These will be the clustering variables. Since we are using a **K-means** algorithm, we first define the optimal number of clusters. With K-means, **9 clusters** have been created and added to the dataframe.

With these new features, we train over the 3 models that we used as our baseline and compare the results with the previous ones.    

### B. Binning

The next Feature Engineering step is **binning** some of the numerical variables (`age`, `balance`, `duration` and `campaign`) following to their quantiles. **Quantile** binning aims to assign the same number of observations to each bin. In the following steps we try binning with various numbers of quantiles:   
   
*  3 bins (dividing the data in 3 quantiles of approx **33%** each)    
*  4 bins (quartiles of **25%**)    
*  5 bins (quantiles of **20%**)   
* 10 bins (quantiles of **10%**)   

The clusters now being found based on Train Set A, can be associated to the points of Train Set B. 

After the binning, we will **one-hot-encode** the categorical variables generated. 

</br> 


***

## Feature Selection with Lasso and RFE  

After doing all the necessary **feature pre processing**,  **feature engineering** and developing our initial **models**, it is about time to start experimenting with some **feature selection methodologies**. In our case we follow the below two methods:    

*  Feature Selection using **Lasso Logistic Regression**.   
*  Feature Selection with **Recursive Feature Elimination**.    

Going deeper in the process, we first take the variables that the *lasso* regression gives us, in order to deal with the problem of multicollinearity as well. In particular, we started our process with **146** variables and the algorithm ended up choosing  **82** of them as important. As a following up step, we apply **RFE**, using *random forest* functions and a cross validation methodology, on the variables given by the lasso regression to get another subset of the important variables, based now on a *tree* method (RandomForest) and we ended up with **21** variables. The justification behind the decision for 21 variables is the plot below that came up as an outcome of the RFE approach.
 
</br>

***

## Tuning  

Now that we have the most efficient set of variables according to our *Feature Selection* approach, we remodel on these set of variables, using once more our 3 main algorithms: **Logistic Regressio**, **Random Forest** and **XGBoost**. However, at this stage in order to improve our results, we apply to each one of the algorithms an extensive **Grid Search** on the parameters that can be tuned and might affect our performance. More in detail we found the below mentioned optimal paramaters for *XGBoost* and *Random Forest*:    

*   **XGBoost**   
   *  max_depth = 6   
   *  gamma = 0     
   *  eta = 0.05   
   *  colsample_bytree = 1    
   *  min_child_weight = 3   
   *  subsample = 1   
   
*   **Random Forest**    
   *  mtry = 4    
   *  splitrule = gini    
   *  min.node.size = 9


</br>

***

## Stacking   

At this stage we have optimized our *variable set* and we have tuned our algorithms through a *Grid Search*. There is another option that we want to try and it is the **stacking models**.  As an initial step to decide whether stacking would make sense or not, we gather all our previous predictions and we plot a *correlation matrix*. The reason behind this, is the fact that if our predictions are *uncorrelated* between each other or at least have low correlation, then the models that generate them are capturing *different aspects* of the validation set, so it makes sense to combine them through a stacking approach. Based on the matrix below, the models that seem better to combine are the ones that generated the predictions based on the **binning** feature engineering step, as the correlation between them is around 50%. However, in order to have a more complete modelling approach we want to stack all the possible combinations and check their performance immediately after. More in detail we follow the below mentioned process:    

*  We create the below mentioned *stacking categories*:   
  *  Baseline modelling    
  *  Clustering modelling (FE1)   
  *  Binning modelling (FE2)
  *  RFE - Tuning modelling   
         
*  For each of the categories we stack the corresponding models with *3 different algorithms*:    
  *  **Logistic Regression**    
  *  **XGBoost**     
  *  **Random Forest**
 


<br><br>

***

***

## Final Model  

Based on the table shown below, in which we gathered all the necessary metrics to compare our models we decide to go with the **Ranger (RandomForest) algorithm trained on the set of variables that include the Binning** step of the Feature Engineering process and is the one that we choose to create our final submission. The metric that was required is the `Sensitivity`, and was the main driver of our decision.  As a closing observation, we can notice that our models are really good in predicting the **No** target variable as we manage to achieve a high *Precision*, but they have harder times *(comparing to the NOs)* adjusting their performance on the **Yes** variable in which they perform well, at around 87% of `Recall`. We can detect that through the various confusion matrices, in which the **True Negative** value is pretty high (thing that justifies the above mentioned observation).     

The overall objective of the model is to identify customers who are responsive to a campaign and will eventually purchase the product. Though the final model does feature high sensitivity (True positive rate), it also has relatively low specificity (True negative rate). This implies a lot of customers targeted by the campaign may ultimately end up rejecting the offer. Ultimately, the trade-off stands between the cost of targeting the client with the campaign, and the increased revenue from capturing the client. In this particular case, one can assume the cost of running the campaign is only a small fraction of the **Customer Life-time Value**. Therefore, it makes sense to provide **an aggressive rather than conservative model**, since the cost of the campaign may only involve customer service labor at relatively low wages. In other settings where the cost of a false positive is highler relatively to the benefit of the true positive, a more conservative option should be adopted.   
   
*   Among those that we don't call: all are not responsive anyways
*   But among those we don't call: we have a lot of no's

<iframe src ="http://ashomah.shinyapps.io/model_dash" height=900px width=100% position="center" frameborder="0" />


<br><br>

***
###### *Nayla Fakhoury | Martin Hofbauer | Andres Llerena | Francesca Manoni | Paul Jacques-Mignault | Ashley O'Mahony | Stavros Tsentemeidis*
###### *O17 (Group G) | Master in Big Data and Business Analytics | Oct 2018 Intake | IE School of Human Sciences and Technology*

***















